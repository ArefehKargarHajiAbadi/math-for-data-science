{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4db1136-1ecb-413e-a2d7-c5ceed77e9a4",
   "metadata": {},
   "source": [
    "#  Information Theory\n",
    "\n",
    "## Objective\n",
    "Learn how to measure **uncertainty**, **information**, and **dependency** between random variables using concepts like entropy, mutual information, and KL divergence — essential for machine learning and data science.\n",
    "\n",
    "---\n",
    "\n",
    "##  Table of Contents\n",
    "1. [Introduction to Uncertainty: Shannon Entropy](#Entropy)  \n",
    "2. [Joint and Conditional Entropy](#JointEntropy)  \n",
    "3. [Mutual Information](#MutualInformation)  \n",
    "4. [Limits of Correlation](#Correlation)  \n",
    "5. [Related Concepts: KL Divergence & Cross Entropy](#KL)  \n",
    "6. [Summary and Applications](#Summary)\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Introduction to Uncertainty: Shannon Entropy <a name=\"Entropy\"></a>\n",
    "\n",
    "Entropy measures **the uncertainty or randomness** in a random variable $X$.  \n",
    "It tells how much “information” is gained on average when observing $X$.\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_i P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "- High entropy → more randomness (e.g., fair coin toss)  \n",
    "- Low entropy → more predictability (e.g., biased coin)\n",
    " *Units*: bits (base 2 logarithm)\n",
    "\n",
    "**Example (concept):**\n",
    "- Fair coin: $H = 1$\n",
    "- Always heads: $H = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Joint and Conditional Entropy <a name=\"JointEntropy\"></a>\n",
    "\n",
    "**Joint Entropy ($H(X, Y)$)**  \n",
    "Represents the uncertainty of a pair of random variables:\n",
    "\n",
    "$$\n",
    "H(X, Y) = - \\sum_{x,y} P(x, y) \\log_2 P(x, y)\n",
    "$$\n",
    "\n",
    "**Conditional Entropy ($H(X|Y)$)**  \n",
    "Measures the uncertainty in $X$ given that $Y$ is known:\n",
    "\n",
    "$$\n",
    "H(X|Y) = H(X, Y) - H(Y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Mutual Information <a name=\"MutualInformation\"></a>\n",
    "\n",
    "Mutual information quantifies how much knowing one variable **reduces uncertainty** about another.\n",
    "\n",
    "$$\n",
    "I(X; Y) = H(X) + H(Y) - H(X, Y)\n",
    "$$\n",
    "\n",
    "- $I(X; Y) = 0$ → variables are independent  \n",
    "- Higher $I$ → stronger dependency (linear or nonlinear)\n",
    "\n",
    "Applications:\n",
    "- Used as **loss functions** in machine learning (e.g., classification, NLP)\n",
    "\n",
    "---\n",
    "\n",
    "### 6.6 Summary and Applications <a name=\"Summary\"></a>\n",
    "\n",
    "| Concept | Measures | Application |\n",
    "|:--|:--|:--|\n",
    "| Entropy | Uncertainty | Data compression, feature entropy |\n",
    "| Mutual Information | Dependency | Feature selection, clustering |\n",
    "| KL Divergence | Difference between distributions | Variational inference |\n",
    "| Cross Entropy | Encoding cost | Classification loss (deep learning) |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
